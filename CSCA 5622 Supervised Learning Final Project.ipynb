{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning: Supervised Learning Final Project\n",
    "\n",
    "\n",
    "For this final project I have decided to attempt to predict how the revenue of a movie based on certain factors.\n",
    "\n",
    "##### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "#import winsound #Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Collect the data\n",
    "\n",
    "I downloaded a csv file that has the TMDB (The Movie Database) IDs of around 45000 movies from this Kaggle [dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/data?select=links.csv). Here is what the data looks like:\n",
    "\n",
    "movieId,imdbId,tmdbId  \n",
    "1,0114709,862  \n",
    "2,0113497,8844  \n",
    "3,0113228,15602\n",
    "\n",
    "I need to create an array of just the TMDB IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_ids = []\n",
    "\n",
    "with open('Data/links.csv', mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        tmdb_ids.append(row['tmdbId'])\n",
    "    \n",
    "print(tmdb_ids[:10]) #Print the first 10 IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the TMDB API to extract data for each movie ID.\n",
    "\n",
    "I did this by saving the data to a jsonl file (JSON Lines) file. I chose this approach because I had trouble going through the entire 45000 movies list without getting some error with connection to their servers. Whenever there would be a single error the entire previous data was wiped, so I decided to make this new approach by update a jsonl file every 100 movies instead of all at once.\n",
    "\n",
    "Some movies in the data that I downloaded did not have a TMDB ID, so I saved their indices to the 'bad_indices.jsonl' file so I knew exactly which ones they were.\n",
    "\n",
    "## Important!\n",
    "In order to run this yourself you need your own free TMDB API key, You can get one here: https://www.themoviedb.org. Once you have your api key update this line in the .env file with your api key: TMDB_API_KEY='your_api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"TMDB_API_KEY\")\n",
    "base_url = \"https://api.themoviedb.org/3/movie/\"\n",
    "\n",
    "# Initialize containers for batch writing\n",
    "movie_data_batch = []\n",
    "bad_indices_batch = []\n",
    "\n",
    "# Loop through each movie ID, fetch data, and batch write every 100 records\n",
    "\n",
    "#To get the full file you would iterate through all the data, but there are over 45000 ids, so it took me hours to get the full data.\n",
    "#If you want to try it out to see how it works I'd recommend just doing a few hundred data points.\n",
    "\n",
    "#for i in range(len(tmdb_ids)): #Use this for loop for the full data. It took me roughly 2 hours to fully run\n",
    "for i in range(130):\n",
    "    movie_id = tmdb_ids[i]\n",
    "    url = f\"{base_url}{movie_id}?api_key={api_key}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            movie_info = {\n",
    "                'id': data['id'],\n",
    "                'title': data['title'],\n",
    "                'budget': data['budget'],\n",
    "                'revenue': data['revenue'],\n",
    "                'release_date': data['release_date'],\n",
    "                'popularity': data['popularity'],\n",
    "                'vote_average': data['vote_average'],\n",
    "                'vote_count': data['vote_count'],\n",
    "                'genres': [genre['name'] for genre in data['genres']],\n",
    "                'runtime': data['runtime'],\n",
    "                'production_companies': [company['name'] for company in data['production_companies']],\n",
    "                'production_countries': [country['name'] for country in data['production_countries']],\n",
    "            }\n",
    "            \n",
    "            #Add movie info to the batch\n",
    "            movie_data_batch.append(movie_info)\n",
    "        \n",
    "        else:\n",
    "            #Add the index of a failed request to the bad indices batch\n",
    "            bad_indices_batch.append(i)\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for movie ID {movie_id} at index {i}\")\n",
    "        #Add the index of a timed-out request to the bad indices batch\n",
    "        bad_indices_batch.append(i)\n",
    "        time.sleep(1)  # Optional: wait 1 second before continuing\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        #Add the index of any other request exception to the bad indices batch\n",
    "        bad_indices_batch.append(i)\n",
    "\n",
    "    #Write batches to files every 100 records\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        print(f'{i} out of {len(tmdb_ids)}')\n",
    "\n",
    "        #Write movie data batch to JSON Lines file\n",
    "        with open('movie_data.jsonl', 'a') as json_file:\n",
    "            for movie in movie_data_batch:\n",
    "                json_file.write(json.dumps(movie) + '\\n')\n",
    "        \n",
    "        #Write bad indices batch to JSON Lines file\n",
    "        with open('bad_indices.jsonl', 'a') as bad_file:\n",
    "            for index in bad_indices_batch:\n",
    "                bad_file.write(json.dumps(index) + '\\n')\n",
    "        \n",
    "        #Clear the batches after writing\n",
    "        movie_data_batch.clear()\n",
    "        bad_indices_batch.clear()\n",
    "\n",
    "#Write any remaining data if the loop ends before reaching a batch of 100\n",
    "if movie_data_batch:\n",
    "    with open('movie_data.jsonl', 'a') as json_file:\n",
    "        for movie in movie_data_batch:\n",
    "            json_file.write(json.dumps(movie) + '\\n')\n",
    "\n",
    "if bad_indices_batch:\n",
    "    with open('bad_indices.jsonl', 'a') as bad_file:\n",
    "        for index in bad_indices_batch:\n",
    "            bad_file.write(json.dumps(index) + '\\n')\n",
    "\n",
    "\n",
    "print(\"Data fetching complete.\")\n",
    "#winsound.Beep(1000, 500)  #A beep to let me know it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that a lot of the data had missing values, so I ran this script to remove all movies with missing values and save it to a file called clean_movie_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields = [\n",
    "    'id', 'title', 'budget', 'revenue', 'release_date', \n",
    "    'popularity', 'vote_average', 'vote_count', \n",
    "    'genres', 'runtime', 'production_companies', 'production_countries'\n",
    "]\n",
    "\n",
    "\n",
    "#Open the input file and a new output file for cleaned data\n",
    "with open('Data/movie_data.jsonl', 'r', encoding='utf-8') as infile, open('clean_movie_data.jsonl', 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        movie = json.loads(line.strip())\n",
    "        \n",
    "        #Check if the movie entry has all required fields with non-empty values\n",
    "        if all(movie.get(field) for field in required_fields):\n",
    "            # rite the entry to the output file if it has full data\n",
    "            outfile.write(json.dumps(movie) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the data into a jsonl file called 'clean_movie_data.jsonl'.  \n",
    "Here is what the end result looks like (Sample of 2 movies out of 7911):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"id\": 862, \"title\": \"Toy Story\", \"budget\": 30000000, \"revenue\": 394436586, \"release_date\": \"1995-10-30\", \"popularity\": 115.812, \"vote_average\": 7.971, \"vote_count\": 18322, \"genres\": [\"Animation\", \"Adventure\", \"Family\", \"Comedy\"], \"runtime\": 81, \"production_companies\": [\"Pixar\"], \"production_countries\": [\"United States of America\"]}\n",
    "#{\"id\": 8844, \"title\": \"Jumanji\", \"budget\": 65000000, \"revenue\": 262821940, \"release_date\": \"1995-12-15\", \"popularity\": 17.805, \"vote_average\": 7.2, \"vote_count\": 10466, \"genres\": [\"Adventure\", \"Fantasy\", \"Family\"], \"runtime\": 104, \"production_companies\": [\"TriStar Pictures\", \"Interscope Communications\", \"Teitler Film\", \"PolyGram Filmed Entertainment\"], \"production_countries\": [\"United States of America\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection is complete, we can now start our exploratory data analysis.  \n",
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data/clean_movie_data.jsonl'\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check to make sure that all the data types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected data types for each field\n",
    "expected_types = {\n",
    "    \"id\": int,\n",
    "    \"title\": str,\n",
    "    \"budget\": int,\n",
    "    \"revenue\": int,\n",
    "    \"release_date\": str,  # We'll check if it's in date format separately\n",
    "    \"popularity\": float,\n",
    "    \"vote_average\": float,\n",
    "    \"vote_count\": int,\n",
    "    \"genres\": list,\n",
    "    \"runtime\": int,\n",
    "    \"production_companies\": list,\n",
    "    \"production_countries\": list\n",
    "}\n",
    "\n",
    "\n",
    "issues = []\n",
    "print(\"Starting data type check...\\n\")\n",
    "\n",
    "for col, expected_type in expected_types.items():\n",
    "    print(f\"Checking column: '{col}'\")\n",
    "\n",
    "    if col in df.columns:\n",
    "        print(f\"  Column '{col}' exists. Expected type: {expected_type.__name__}\")\n",
    "        \n",
    "        #Check if the entire column is of the expected type\n",
    "        type_issues = False\n",
    "        for index, val in df[col].dropna().items():\n",
    "            if not isinstance(val, expected_type):\n",
    "                print(f\"    Type issue found at index {index} in column '{col}'. Value: {val} (Type: {type(val).__name__})\")\n",
    "                type_issues = True\n",
    "                issues.append(f\"Column '{col}' has values that do not match the expected type {expected_type.__name__}.\")\n",
    "                break\n",
    "\n",
    "        if not type_issues:\n",
    "            print(f\"  All values in column '{col}' match the expected type.\\n\")\n",
    "        \n",
    "        # Special handling for release_date to check for valid date format\n",
    "        if col == \"release_date\" and df[col].dtype == 'O':  # 'O' denotes object type\n",
    "            print(f\"  Checking date format for column '{col}'\")\n",
    "            date_issues = False\n",
    "            for index, date_str in df[col].dropna().items():\n",
    "                try:\n",
    "                    pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    print(f\"    Date format issue at index {index} in column '{col}'. Value: {date_str}\")\n",
    "                    date_issues = True\n",
    "                    issues.append(f\"Column 'release_date' contains invalid date formats.\")\n",
    "                    break\n",
    "\n",
    "            if not date_issues:\n",
    "                print(f\"  All values in column '{col}' have a valid date format.\\n\")\n",
    "    else:\n",
    "        print(f\"  Column '{col}' is missing from the DataFrame.\")\n",
    "        issues.append(f\"Column '{col}' is missing from the DataFrame.\")\n",
    "\n",
    "print(\"\\nData type check completed.\\n\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Data Type Issues Found:\")\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"All data types are correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a better understanding of the data. We'll start by calculating the mean, median, etc. for the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\"budget\", \"revenue\", \"popularity\", \"vote_average\", \"vote_count\", \"runtime\"]\n",
    "\n",
    "summary_stats = df[numerical_columns].describe().T  # Transpose for readability\n",
    "summary_stats = summary_stats.round(0).astype(int) #Round and get rid of scientific notation\n",
    "summary_stats = summary_stats.map(lambda x: f\"{x:,}\") #Add commas to make big numbers more readable\n",
    "\n",
    "\n",
    "print(\"Summary Statistics for Numerical Columns:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty interesting! Just out of curiosity, let me see which movie made ~3 billion dollars in revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_revenue_movie = df[df['revenue'] == 2923706026]\n",
    "print(max_revenue_movie[['title', 'revenue', 'release_date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a good movie. Let's make some remarks about the statistics:\n",
    "\n",
    "-The average budget is around $24.8 million and has a high standard deviation of $35.6 million, which indicates a very high range in budgets, especially since the standard deviation is remarkably higher than the mean.  \n",
    "-Since the median budget is $12 million and 75% of movies have a budget of $30 million or less, it suggests that a small amount of movies skew the average upwards.\n",
    "\n",
    "-We can notice a similar thing about the revenue: a noticably higher standard deviation than the mean, suggesting high variability.\n",
    "\n",
    "-The average popularity score is 24, but the standard deviation is 22, which indicates that some movies have extremely high popularity scores compared to the average\n",
    "\n",
    "-The ratings seems pretty consistent, with the average rating being 6.4 and a relatively low standard deviation of 1\n",
    "\n",
    "Let's see which movie had the highest popularity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_movie = df.loc[df['popularity'].idxmax()]\n",
    "print(most_popular_movie[['title', 'revenue', 'release_date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't seen Gladiatory yet, I'll watch it tonight!\n",
    "\n",
    "Now let's make histograms for budget, revenue, and popularity to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = [\"budget\", \"revenue\", \"popularity\"]\n",
    "\n",
    "\n",
    "for col in columns_to_plot:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[col].dropna(), bins=30)  # Drop NaNs to avoid issues in plotting\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we transition into building a supervised learning model, let's look at the correlation plot to identify potential relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[numerical_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True, fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that features like budget, popularity, and vote_count seem to correlate strongly to revenue, which we are trying to predict. Since it seems like the budget, popularity, and vote_count are the most important features by far and including more features might introduce too many unneccesary dimensions, let's only include those in our data (alongside the title)  \n",
    "This code saves only 'title', 'budget', 'popularity', 'vote_count', 'revenue' to a new file called final_movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['title', 'budget', 'popularity', 'vote_count', 'revenue']]\n",
    "df_final = df_final.dropna()\n",
    "\n",
    "print(\"Cleaned Data:\")\n",
    "print(df_final)\n",
    "\n",
    "df_final.to_csv('final_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Supervised Learning Models\n",
    "\n",
    "I have decided to use three methods: Random Forest Regressor, Gradient Boosting Regressor, and Linear Regression. I chose a them because the methods can capture complex relationships between the features and the target. This is especially useful in my dataset, since there are very complicated and intricate relationships between certain features and how well a movie is going to do. I chose Linear Regression because I wanted to see how these more complicated models would do against a simpler one.\n",
    "\n",
    "\n",
    "##### Setting up the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['budget', 'popularity', 'vote_count']\n",
    "target = 'revenue'\n",
    "\n",
    "X = df_final[features]\n",
    "y = df_final[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2 Score): {r2}\")\n",
    "\n",
    "#Hyperparameter Tuning with RandomizedSearchCV\n",
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "#Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid, n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "#Fit RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_rf_model = random_search.best_estimator_\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "\n",
    "#Evaluate the best model on the test set\n",
    "y_pred_best = best_rf_model.predict(X_test)\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Optimized Mean Absolute Error (MAE): {mae_best}\")\n",
    "print(f\"Optimized R-squared (R2 Score): {r2_best}\")\n",
    "\n",
    "absolute_errors_best = np.abs(y_test - y_pred_best)\n",
    "median_absolute_error_best = np.median(absolute_errors_best)\n",
    "print(f\"Optimized Median Absolute Error: {median_absolute_error_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to do hyperparameter turning and pick the best model after randomly searching the parameter space, testing 20 different combinations. Here were my results:\n",
    "\n",
    "Mean Absolute Error (MAE): 35,186,864  \n",
    "Median Absolute Error: 14,117,142  \n",
    "R-squared (R2 Score): 0.7545467916896331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2 Score): {r2}\")\n",
    "\n",
    "# Hyperparameter Tuning with RandomizedSearchCV\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "\n",
    "y_pred_best = best_xgb_model.predict(X_test)\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Optimized Mean Absolute Error (MAE): {mae_best}\")\n",
    "print(f\"Optimized R-squared (R2 Score): {r2_best}\")\n",
    "\n",
    "absolute_errors_best = np.abs(y_test - y_pred_best)\n",
    "median_absolute_error_best = np.median(absolute_errors_best)\n",
    "print(f\"Optimized Median Absolute Error: {median_absolute_error_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a similar thing for the XGBoost Regressor, here were my results:\n",
    "\n",
    "Mean Absolute Error (MAE): 34,807,785  \n",
    "Median Absolute Error: 14146072  \n",
    "R-squared (R2 Score): 0.7556353807449341  \n",
    "\n",
    "For fun, I decided to fit a linear regression model to the data to see how well it would do compared to the other more advanced methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2 Score): {r2}\")\n",
    "\n",
    "coefficients = pd.DataFrame({'Feature': features, 'Coefficient': linear_model.coef_})\n",
    "print(\"Feature Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "absolute_errors = np.abs(y_test - y_pred)\n",
    "\n",
    "median_absolute_error = np.median(absolute_errors)\n",
    "print(f\"Median Absolute Error: {median_absolute_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results:\n",
    "\n",
    "Mean Absolute Error (MAE): 40,286,811  \n",
    "Median Absolute Error: 18466363    \n",
    "R-squared (R2 Score): 0.7042155464769719\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Here is a summary of my best results for each method:\n",
    "\n",
    "Random Forest Regressor:  \n",
    "MAE: 35,186,864  \n",
    "Median Absolute Error: 14,117,142  \n",
    "R-squared: 0.754  \n",
    "\n",
    "XGBoost:  \n",
    "MAE: 34807785  \n",
    "Median Absolute Error: 14146072  \n",
    "R-squared: 0.755  \n",
    "\n",
    "Linear Regression:  \n",
    "MAE: 40,286,811  \n",
    "Median Absolute Error: 18466363    \n",
    "R-squared: 0.704\n",
    "\n",
    "My best method was the XGBoost, but the Random Forest Regressor came close. It might seem like the MAE is off by a lot, since it is almost 35 million, but I would say these are quite good results. Reflecting back to the graph of revenue, we can see a very large variation. The statistics back it up, since we had a mean of 68,887,873 but an extremely large standard deviation of 146,351,668. This means if a movie comes along that unsuspectingly does very well and the model doesn't know this, it could easily be off by hundreds of millions in its prediction. We can prove this suspicion by noticing that the median absolute error is less than half of the mean absolute error. This means that the main error happened from outliers, while the majority of the error wasn't that bad. The model is trying to predict a very complicated market that is influenced by many real-world factors. For these reasons, I am very content with the results of my models.\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "In this project, I explored the factors that contribute to movie revenue by developing supervised learning algorithms that predict the revenue of a movie. I downloaded TMDB IDs for movies, and using those I downloaded data, cleaned, and analyzed it. I used the models XGBoost, Random Forest, and Linear Regression to predict the revenue of movies. From these models, XGBoost made the most accurate predictions by achieving the highest accuracy and the lowest mean absolute error. I am impressed with how accurately my results were able to model a complicated market. Perhaps if Isaac Newton had access to supervised learning, he would have never said \"I can calculate the motion of heavenly bodies, but not the madness of people.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
